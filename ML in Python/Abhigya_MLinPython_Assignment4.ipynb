{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement your own logistic regression and classify the iris data into setosa or non-setosa. You are supposed to write your own logit function and implement gradient descent to learn optimal weights. Then using this weight classify the entire data set as setosa or non-setosa. We encourage you not to use logistic regression implementation of scikit learn package. (If you are facing too much difficulty during implementation you can use packages no marks will be deducted for that. However, please try your best to avoid using packages. ) Report how much accuracy you got. You can try your logistic regression code on some other dataset as well for binary classification. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width           class\n",
       "0             5.1          3.5           1.4          0.2     Iris-setosa\n",
       "1             4.9          3.0           1.4          0.2     Iris-setosa\n",
       "2             4.7          3.2           1.3          0.2     Iris-setosa\n",
       "3             4.6          3.1           1.5          0.2     Iris-setosa\n",
       "4             5.0          3.6           1.4          0.2     Iris-setosa\n",
       "..            ...          ...           ...          ...             ...\n",
       "145           6.7          3.0           5.2          2.3  Iris-virginica\n",
       "146           6.3          2.5           5.0          1.9  Iris-virginica\n",
       "147           6.5          3.0           5.2          2.0  Iris-virginica\n",
       "148           6.2          3.4           5.4          2.3  Iris-virginica\n",
       "149           5.9          3.0           5.1          1.8  Iris-virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = pd.read_csv('iris.csv')\n",
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the labels to predict\n",
    "Y = list((iris['class'] == 'Iris-setosa').astype(int))\n",
    "Y = np.asarray(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the features of the data\n",
    "X = iris.iloc[:,:4].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig(x):\n",
    "    o = 1 / (1 + np.exp(-x))\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_fun(y_hat, y):\n",
    "    \n",
    "    c = (- y * np.log(y_hat)) - ((1 - y) * np.log(1 - y_hat))\n",
    "    \n",
    "    return c.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reg(W, X, B, Y, eta):\n",
    "    #number of samples\n",
    "    m = X.shape[0]\n",
    "    #Getting the y_hat value, This will be a value between 0 and 1\n",
    "    Y_hat = sig(np.dot(X, W) + B)\n",
    "    \n",
    "    #Derivative of sigmoid function\n",
    "    cost = cost_fun(Y_hat, Y)\n",
    "    \n",
    "    #Gradient\n",
    "    dW = 1/m * np.dot(X.T, (Y_hat - Y))\n",
    "    dB = 1/m * np.sum(Y_hat - Y)\n",
    "    \n",
    "    #Update\n",
    "    W = W - eta * dW\n",
    "    B = B - eta * dB\n",
    "\n",
    "    return W, B, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, eta, epochs):\n",
    "    \n",
    "    #Getting number of features\n",
    "    p = X.shape[1]\n",
    "    \n",
    "    #Initializing W and B with random numbers\n",
    "    #W = np.zeros(p)\n",
    "    W = np.random.rand(p)\n",
    "    #B = 0\n",
    "    B = np.random.rand()\n",
    "    \n",
    "    #Shuffling the data\n",
    "    #To avoid overfitting\n",
    "    shuffler = np.random.permutation(len(X))\n",
    "    X_ = X[shuffler]\n",
    "    Y_ = Y[shuffler]\n",
    "    \n",
    "    cost_list = []\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        W, B, cost = log_reg(W, X_, B, Y_, eta)\n",
    "        cost_list.append(cost)\n",
    "\n",
    "    return W, B, cost_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(X, Y, W, B):\n",
    "    \n",
    "    Y_h = sig(np.dot(X, W) + B)\n",
    "    \n",
    "    pred = Y_h > 0.5\n",
    "    actual = Y > 0.5\n",
    "    #Using this to get total number of correct predictions\n",
    "    correct_pred = sum((pred == actual) * 1)\n",
    "    \n",
    "    accuracy = (correct_pred/ X.shape[0]) * 100\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Logistic Regression!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1, B1, cost_list1 = train(X, Y, 0.1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Achieved accuracy is:  100.0\n"
     ]
    }
   ],
   "source": [
    "#Checking accuracy\n",
    "acc1 = calc_accuracy(X, Y, W1, B1)\n",
    "print(\"Achieved accuracy is: \", acc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Achieved accuracy on shuffled data is:  100.0\n"
     ]
    }
   ],
   "source": [
    "#Checking accuracy on shuffled data.\n",
    "shuffler = np.random.permutation(len(X))\n",
    "X_ = X[shuffler]\n",
    "Y_ = Y[shuffler]\n",
    "\n",
    "acc2 = calc_accuracy(X_, Y_, W1, B1)\n",
    "print(\"Achieved accuracy on shuffled data is: \", acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting error over the epochs\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(cost_list1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lower learning rate decreases the accuracy. Looking at the plot for cost, it can be seen that lesser epochs can also achieve similar accuracy.\n",
    "Iris dataset is easy to classify. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for test data is:  100.0\n"
     ]
    }
   ],
   "source": [
    "#Checking by dividing dataset into training and testing\n",
    "\n",
    "#Taking the shuffled dataset\n",
    "\n",
    "W2, B2, cost_list2 = train(X_[:120], Y_[:120], 0.1, 100)\n",
    "\n",
    "acc3 = calc_accuracy(X_[120:], Y_[120:], W2, B2)\n",
    "print(\"Accuracy for test data is: \", acc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting error over the epochs\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(cost_list2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You are familiar with the matrix notation and linear algebra. Using the mathematical procedure (discussed on unit, 9) of feature selection implement your own feature selection algorithm to select 5 best features of OnlineNewsPopularity data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>...</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>219</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.680365</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.70</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>255</td>\n",
       "      <td>0.604743</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.791946</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.913725</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.70</td>\n",
       "      <td>-0.118750</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>211</td>\n",
       "      <td>0.575130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.393365</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.466667</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>531</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.665635</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.404896</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-0.369697</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>1072</td>\n",
       "      <td>0.415646</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.540890</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>4.682836</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.220192</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39639</th>\n",
       "      <td>11</td>\n",
       "      <td>346</td>\n",
       "      <td>0.529052</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.684783</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.523121</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>-0.260000</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39640</th>\n",
       "      <td>12</td>\n",
       "      <td>328</td>\n",
       "      <td>0.696296</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.885057</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>48</td>\n",
       "      <td>4.405488</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.70</td>\n",
       "      <td>-0.211111</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39641</th>\n",
       "      <td>10</td>\n",
       "      <td>442</td>\n",
       "      <td>0.516355</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.644128</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>5.076923</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.356439</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>1900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39642</th>\n",
       "      <td>6</td>\n",
       "      <td>682</td>\n",
       "      <td>0.539493</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.692661</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.975073</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.205246</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.012500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39643</th>\n",
       "      <td>10</td>\n",
       "      <td>157</td>\n",
       "      <td>0.701987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.471338</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39644 rows Ã— 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       n_tokens_title  n_tokens_content  n_unique_tokens  n_non_stop_words  \\\n",
       "0                  12               219         0.663594               1.0   \n",
       "1                   9               255         0.604743               1.0   \n",
       "2                   9               211         0.575130               1.0   \n",
       "3                   9               531         0.503788               1.0   \n",
       "4                  13              1072         0.415646               1.0   \n",
       "...               ...               ...              ...               ...   \n",
       "39639              11               346         0.529052               1.0   \n",
       "39640              12               328         0.696296               1.0   \n",
       "39641              10               442         0.516355               1.0   \n",
       "39642               6               682         0.539493               1.0   \n",
       "39643              10               157         0.701987               1.0   \n",
       "\n",
       "       n_non_stop_unique_tokens  num_hrefs  num_self_hrefs  num_imgs  \\\n",
       "0                      0.815385          4               2         1   \n",
       "1                      0.791946          3               1         1   \n",
       "2                      0.663866          3               1         1   \n",
       "3                      0.665635          9               0         1   \n",
       "4                      0.540890         19              19        20   \n",
       "...                         ...        ...             ...       ...   \n",
       "39639                  0.684783          9               7         1   \n",
       "39640                  0.885057          9               7         3   \n",
       "39641                  0.644128         24               1        12   \n",
       "39642                  0.692661         10               1         1   \n",
       "39643                  0.846154          1               1         0   \n",
       "\n",
       "       num_videos  average_token_length  ...  min_positive_polarity  \\\n",
       "0               0              4.680365  ...               0.100000   \n",
       "1               0              4.913725  ...               0.033333   \n",
       "2               0              4.393365  ...               0.100000   \n",
       "3               0              4.404896  ...               0.136364   \n",
       "4               0              4.682836  ...               0.033333   \n",
       "...           ...                   ...  ...                    ...   \n",
       "39639           1              4.523121  ...               0.100000   \n",
       "39640          48              4.405488  ...               0.136364   \n",
       "39641           1              5.076923  ...               0.136364   \n",
       "39642           0              4.975073  ...               0.062500   \n",
       "39643           2              4.471338  ...               0.100000   \n",
       "\n",
       "       max_positive_polarity  avg_negative_polarity  min_negative_polarity  \\\n",
       "0                       0.70              -0.350000                 -0.600   \n",
       "1                       0.70              -0.118750                 -0.125   \n",
       "2                       1.00              -0.466667                 -0.800   \n",
       "3                       0.80              -0.369697                 -0.600   \n",
       "4                       1.00              -0.220192                 -0.500   \n",
       "...                      ...                    ...                    ...   \n",
       "39639                   0.75              -0.260000                 -0.500   \n",
       "39640                   0.70              -0.211111                 -0.400   \n",
       "39641                   0.50              -0.356439                 -0.800   \n",
       "39642                   0.50              -0.205246                 -0.500   \n",
       "39643                   0.50              -0.200000                 -0.200   \n",
       "\n",
       "       max_negative_polarity  title_subjectivity  title_sentiment_polarity  \\\n",
       "0                  -0.200000            0.500000                 -0.187500   \n",
       "1                  -0.100000            0.000000                  0.000000   \n",
       "2                  -0.133333            0.000000                  0.000000   \n",
       "3                  -0.166667            0.000000                  0.000000   \n",
       "4                  -0.050000            0.454545                  0.136364   \n",
       "...                      ...                 ...                       ...   \n",
       "39639              -0.125000            0.100000                  0.000000   \n",
       "39640              -0.100000            0.300000                  1.000000   \n",
       "39641              -0.166667            0.454545                  0.136364   \n",
       "39642              -0.012500            0.000000                  0.000000   \n",
       "39643              -0.200000            0.333333                  0.250000   \n",
       "\n",
       "       abs_title_subjectivity  abs_title_sentiment_polarity  shares  \n",
       "0                    0.000000                      0.187500     593  \n",
       "1                    0.500000                      0.000000     711  \n",
       "2                    0.500000                      0.000000    1500  \n",
       "3                    0.500000                      0.000000    1200  \n",
       "4                    0.045455                      0.136364     505  \n",
       "...                       ...                           ...     ...  \n",
       "39639                0.400000                      0.000000    1800  \n",
       "39640                0.200000                      1.000000    1900  \n",
       "39641                0.045455                      0.136364    1900  \n",
       "39642                0.500000                      0.000000    1100  \n",
       "39643                0.166667                      0.250000    1300  \n",
       "\n",
       "[39644 rows x 59 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('OnlineNewsPopularity.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the linear regression model to calculate SSE and be able to perform feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_error(y_h, y):\n",
    "    error = (y_h - y)\n",
    "    error = np.square(error)\n",
    "    error = sum(error)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Actual_reg(x_train, y_train):\n",
    "    #Transposing x\n",
    "    x_t = x_train.T\n",
    "    s = np.matmul(x_t,x_train)\n",
    "    #Finding inverse\n",
    "    si = np.linalg.inv(s)\n",
    "    b = np.matmul(si,x_t)\n",
    "    #Getting final w_h\n",
    "    w_h = np.matmul(b,y_train)\n",
    "    return w_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def Linear_reg(Data, Resp, test_size):\n",
    "    #Splitting data into testing and training data.\n",
    "    x_train, x_test, y_train, y_test = train_test_split(Data, Resp, test_size = test_size, random_state=1, shuffle = False)\n",
    "    #Calculating the best possible w_h \n",
    "    w_h = Actual_reg(x_train, y_train)\n",
    "    #Getting the predicted values\n",
    "    y_h_train = np.matmul(x_train, w_h)\n",
    "    y_h_test = np.matmul(x_test, w_h)\n",
    "    #Calculating the error\n",
    "    train_error = calc_error(y_h_train, y_train)\n",
    "    test_error = calc_error(y_h_test, y_test)\n",
    "    return train_error, test_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Linear Regression on the Data without feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataX = df.iloc[:,:N-1].values\n",
    "\n",
    "DataY = df['shares'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error with all the features:  330026266486217.94\n",
      "Testing error with all the features:  3.1063201602565734e+18\n"
     ]
    }
   ],
   "source": [
    "train_error1, test_error1 = Linear_reg(DataX, DataY, 0.8)\n",
    "\n",
    "print(\"Training error with all the features: \", train_error1)\n",
    "print(\"Testing error with all the features: \", test_error1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select 5 best features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach: Find how each feature affects the error. By taking one feature at a time. Then combine features with lowest errors till best sse is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSE_list = {}\n",
    "#Getting feature wise list of errors\n",
    "\n",
    "for i in range(N-1):\n",
    "    x = df.iloc[:, i].values\n",
    "    x = x.reshape([x.shape[0], 1])\n",
    "    tr, te = Linear_reg(x, DataY, 0.8)\n",
    "    \n",
    "    SSE_list[i] = tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting the features on basis of SSE\n",
    "Sorted_SSE = {k: v for k, v in sorted(SSE_list.items(), key=lambda item: item[1])}\n",
    "\n",
    "#Taking first 5 items!\n",
    "first_feature = list(Sorted_SSE.items())[0]\n",
    "#Next 4 features\n",
    "out = dict(list(Sorted_SSE.items())[1: 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For only the best 5 features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Train SSE after taking best 5 features:  1239757189288.8093\n",
      "Final Test SSE after taking best 5 features:  4069635422271.789\n"
     ]
    }
   ],
   "source": [
    "n, er = first_feature\n",
    "x = df.iloc[:, n].values\n",
    "x = x.reshape([x.shape[0], 1])\n",
    "\n",
    "SSE = er\n",
    "\n",
    "for k in out:\n",
    "    x_new = df.iloc[:, k].values\n",
    "    x_new = x_new.reshape([x_new.shape[0], 1])\n",
    "    x = np.append(x, x_new, 1)\n",
    "    \n",
    "    tr, te = Linear_reg(x, DataY, 0.8)\n",
    "\n",
    "print(\"Final Train SSE after taking best 5 features: \", tr)\n",
    "print(\"Final Test SSE after taking best 5 features: \", te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If had to find the optimal number of features for best results with Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best results with Linear Regression can be achieved with  21 features\n",
      "Lowest Training SSE achieveable is  1236256175995.8584\n",
      "Lowest Testing SSE achieveable is  4075838840973.5586\n"
     ]
    }
   ],
   "source": [
    "n, er = first_feature\n",
    "x = df.iloc[:, n].values\n",
    "x = x.reshape([x.shape[0], 1])\n",
    "\n",
    "SSE = er\n",
    "\n",
    "count = 1\n",
    "\n",
    "for k in dict(list(Sorted_SSE.items())[1:]):\n",
    "    x_new = df.iloc[:, k].values\n",
    "    x_new = x_new.reshape([x_new.shape[0], 1])\n",
    "    x = np.append(x, x_new, 1)\n",
    "    \n",
    "    tr, te = Linear_reg(x, DataY, 0.8)\n",
    "\n",
    "    diff = SSE - tr\n",
    "    SSE = tr\n",
    "    \n",
    "    count+= 1\n",
    "    \n",
    "    if abs(diff) < 20000:\n",
    "        break\n",
    "        \n",
    "print(\"Best results with Linear Regression can be achieved with \", count, \"features\")\n",
    "print(\"Lowest Training SSE achieveable is \", SSE)\n",
    "print(\"Lowest Testing SSE achieveable is \", te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the error is too large, a large Threshhold has been taken.!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Sklearn method!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhigyaurja/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass n_features_to_select=5 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    }
   ],
   "source": [
    "model=LinearRegression(fit_intercept=True,normalize=True,n_jobs=-1)\n",
    "\n",
    "# create the RFE model and selecting 5 attributes\n",
    "\n",
    "rfe = RFE(model, 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RFE(estimator=LinearRegression(n_jobs=-1, normalize=True),\n",
       "    n_features_to_select=5)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfe.fit(DataX, DataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x = df.iloc[:,:N-1]  ##Same as DataX but in dataframe form\n",
    "New_X = new_x[new_x.columns[rfe.support_]].values ##Finding the columns which are best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting in the same way as our own dataset was split for checking.\n",
    "x_train, x_test, y_train, y_test = train_test_split(New_X, DataY, test_size = 0.8,\\\n",
    "                                                    random_state=1, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = reg.predict(x_test)\n",
    "y_pred2 = reg.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calc_error finds the sum of squared errors.\n",
    "e1 = calc_error(y_pred1, y_test)\n",
    "e2 = calc_error(y_pred2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SSE with RFE method:  1242875223960.4448\n",
      "Testing SSE with RFE method:  4092634855356.1914\n"
     ]
    }
   ],
   "source": [
    "print(\"Training SSE with RFE method: \", e2)\n",
    "print(\"Testing SSE with RFE method: \", e1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
